{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP - Traitement Automatique du Langage Naturel\n",
    "\n",
    "Notre librarie de code préférée pour le traitement du langage naturel est [SpaCy](../tools/spacy.ipynb), et nous lui avons d'ailleurs dédié un bloc-note Jupyter qui démontre les bases.\n",
    "\n",
    "Les [transformers](https://arxiv.org/abs/1706.03762) ont changé les paradigmes du traitement du langage naturel. Il existe de nombreux modèles anglais et multilingues, mais peu de modèles français, l'inconvénient des modèles multilingues étant principalement leur taille, et accessoirement les faiblesses de la part du corpus d'entrainement dédiée au français. Nous avons donc souhaité rendre ici hommage aux rares modèles dédiés au français, que sont [Camembert](https://huggingface.co/almanach), [Croissant](https://huggingface.co/croissantllm), [Flaubert](https://huggingface.co/flaubert), [Vigogne](https://huggingface.co/bofenghuang), et bientôt Lucie.\n",
    "\n",
    "[![Index](https://img.shields.io/badge/Index-blue)](../index.ipynb)\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/digillia/Digillia-Colab/blob/main/algorithms/nlp-basics.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Supprimer les commentaires pour installer (requirements.txt)\n",
    "# !pip3 install -q -U torch\n",
    "# !pip3 install -q -U transformers\n",
    "\n",
    "# À installer dans tous les cas pour Google Colab et Github\n",
    "if 'google.colab' in sys.modules or 'CI' in os.environ:\n",
    "    !pip3 install -q -U sacremoses # requis par Flaubert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Camembert\n",
    "camembert_name = \"camembert/camembert-base\"\n",
    "# Croissant\n",
    "croissant_name = \"croissantllm/CroissantLLMBase\"\n",
    "# Flaubert\n",
    "flaubert_name = \"flaubert/flaubert_base_cased\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, CamembertTokenizer, FlaubertTokenizer\n",
    "\n",
    "# Camembert\n",
    "camembert_tokenizer = CamembertTokenizer.from_pretrained(camembert_name)\n",
    "# Croissant\n",
    "# croissant_tokenizer = AutoTokenizer.from_pretrained(croissant_name)\n",
    "# Flaubert\n",
    "flaubert_tokenizer = FlaubertTokenizer.from_pretrained(flaubert_name, do_lowercase=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CamembertTokenizer(name_or_path='camembert/camembert-base', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>', 'additional_special_tokens': ['<s>NOTUSED', '</s>NOTUSED', '<unk>NOTUSED']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<s>NOTUSED\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>NOTUSED\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t4: AddedToken(\"<unk>NOTUSED\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t5: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t6: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32004: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(camembert_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(croissant_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FlaubertTokenizer(name_or_path='flaubert/flaubert_base_cased', vocab_size=68729, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '</s>', 'mask_token': '<special1>', 'additional_special_tokens': ['<special0>', '<special1>', '<special2>', '<special3>', '<special4>', '<special5>', '<special6>', '<special7>', '<special8>', '<special9>']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t4: AddedToken(\"<special0>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t5: AddedToken(\"<special1>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t6: AddedToken(\"<special2>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t7: AddedToken(\"<special3>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t8: AddedToken(\"<special4>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t9: AddedToken(\"<special5>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t10: AddedToken(\"<special6>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t11: AddedToken(\"<special7>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t12: AddedToken(\"<special8>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t13: AddedToken(\"<special9>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(flaubert_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> est dans le vocabulaire de camembert/camembert-base: 5\n",
      "chat est dans le vocabulaire de camembert/camembert-base: 8734\n",
      "chien n'est pas dans le vocabulaire de camembert/camembert-base\n",
      "balle n'est pas dans le vocabulaire de camembert/camembert-base\n",
      "\n",
      "<s> est dans le vocabulaire de flaubert/flaubert_base_cased: 0\n",
      "chat est dans le vocabulaire de flaubert/flaubert_base_cased: 8830\n",
      "chien n'est pas dans le vocabulaire de flaubert/flaubert_base_cased\n",
      "balle n'est pas dans le vocabulaire de flaubert/flaubert_base_cased\n"
     ]
    }
   ],
   "source": [
    "def check_vocab(name='<s>', tokenizer=camembert_tokenizer):\n",
    "    vocab = tokenizer.get_vocab()\n",
    "    try:\n",
    "        id = vocab[name]\n",
    "        print(f\"{name} est dans le vocabulaire de {tokenizer.name_or_path}: {id}\")\n",
    "    except KeyError:\n",
    "        print(f\"{name} n'est pas dans le vocabulaire de {tokenizer.name_or_path}\")\n",
    "\n",
    "# Camembert\n",
    "check_vocab()\n",
    "check_vocab('chat')\n",
    "check_vocab('chien')\n",
    "check_vocab('balle')\n",
    "# print()\n",
    "# Croissant\n",
    "# check_vocab(tokenizer=croissant_tokenizer)\n",
    "# check_vocab('chat', croissant_tokenizer)\n",
    "# check_vocab('chien', croissant_tokenizer)\n",
    "# check_vocab('balle', croissant_tokenizer)\n",
    "print()\n",
    "# Flaubert\n",
    "check_vocab(tokenizer=flaubert_tokenizer)\n",
    "check_vocab('chat', flaubert_tokenizer)\n",
    "check_vocab('chien', flaubert_tokenizer)\n",
    "check_vocab('balle', flaubert_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, CamembertModel, FlaubertModel\n",
    "\n",
    "# Camembert\n",
    "camembert_model = CamembertModel.from_pretrained(camembert_name)\n",
    "# Croissant\n",
    "# croissant_model = AutoModelForCausalLM.from_pretrained(croissant_name, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "# Flaubert\n",
    "flaubert_model = FlaubertModel.from_pretrained(flaubert_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, CamembertModel, FlaubertModel\n",
    "\n",
    "# Camembert\n",
    "camembert_model = CamembertModel.from_pretrained(camembert_name)\n",
    "# Croissant\n",
    "# croissant_model = AutoModelForCausalLM.from_pretrained(croissant_name, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "# Flaubert\n",
    "flaubert_model = FlaubertModel.from_pretrained(flaubert_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "nlp_fill_camembert = pipeline('fill-mask', model=camembert_name, top_k=10)\n",
    "#nlp_fill_croissant = pipeline('fill-mask', model=croissant_name, top_k=10)\n",
    "nlp_fill_flaubert = pipeline('fill-mask', model=flaubert_name, top_k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/@xiaoouwang/playing-with-camembert-and-flaubert-8c5d40e502a7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inputs = tokenizer(\"I am so tired I could sleep right now. -> Je suis si fatigué que je pourrais m'endormir maintenant.\\nHe is heading to the market. -> Il va au marché.\\nWe are running on the beach. ->\", return_tensors=\"pt\").to(model.device)\n",
    "#tokens = model.generate(**inputs, max_length=100, do_sample=True, top_p=0.95, top_k=60, temperature=0.3)\n",
    "#print(tokenizer.decode(tokens[0]))\n",
    "\n",
    "# remove bos token\n",
    "#inputs = tokenizer(\"Capitales: France -> Paris, Italie -> Rome, Allemagne -> Berlin, Espagne ->\", return_tensors=\"pt\", add_special_tokens=True).to(model.device)\n",
    "#tokens = model.generate(**inputs, max_length=100, do_sample=True, top_p=0.95, top_k=60)\n",
    "#print(tokenizer.decode(tokens[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPO ID                      REPO TYPE SIZE ON DISK NB FILES LAST_ACCESSED LAST_MODIFIED REFS LOCAL PATH                                                                    \n",
      "---------------------------- --------- ------------ -------- ------------- ------------- ---- ----------------------------------------------------------------------------- \n",
      "camembert/camembert-base     model           446.0M        3 22 hours ago  22 hours ago  main /Users/jlchereau/.cache/huggingface/hub/models--camembert--camembert-base     \n",
      "flaubert/flaubert_base_cased model           555.7M        5 22 hours ago  22 hours ago  main /Users/jlchereau/.cache/huggingface/hub/models--flaubert--flaubert_base_cased \n",
      "\n",
      "Done in 0.0s. Scanned 2 repo(s) for a total of \u001b[1m\u001b[31m1.0G\u001b[0m.\n",
      "\u001b[90mGot 2 warning(s) while scanning. Use -vvv to print details.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Pour supprimer les modèles du cache\n",
    "# !pip3 install -q -U \"huggingface_hub[cli]\"\n",
    "# !huggingface-cli scan-cache\n",
    "# !huggingface-cli delete-cache"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
