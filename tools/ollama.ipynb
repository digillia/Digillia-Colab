{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "561de6bb",
   "metadata": {},
   "source": [
    "# ollama\n",
    "\n",
    "[![Index](https://img.shields.io/badge/Index-blue)](../index.ipynb)\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/digillia/Digillia-Colab/blob/main/tools/ollama.ipynb)\n",
    "\n",
    "Like [vLLM](./vllm.ipynb), ollama is a fast and easy-to-use library for LLM inference and serving.\n",
    "\n",
    "Docs:\n",
    "- https://github.com/ollama/ollama\n",
    "- https://ollama.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7a47f1",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "> <p style=\"color:red;\">ollama should be installed and running.<p>\n",
    "\n",
    "Download and install from https://ollama.com/download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df4473a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Supprimer les commentaires pour installer\n",
    "# !pip3 install -qU -r ../requirements.txt\n",
    "\n",
    "# À installer dans tous les cas pour Google Colab et Github\n",
    "if 'google.colab' in sys.modules or 'CI' in os.environ:\n",
    "    !wget https://ollama.ai/install.sh -O /tmp/ollama.sh\n",
    "    !bash /tmp/ollama.sh\n",
    "    !pip3 install -qU ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0457e021",
   "metadata": {},
   "source": [
    "## Exécution en Arrière-Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a787307",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "import os\n",
    "\n",
    "def kill_ollama():\n",
    "    # Kill any existing Ollama processes\n",
    "    try:\n",
    "        subprocess.run(['pkill', 'ollama'], check=False)\n",
    "        time.sleep(2)\n",
    "        print(\"Cleaned up any existing Ollama processes\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Launch Ollama in the background\n",
    "def start_ollama():\n",
    "    try:\n",
    "        # Start Ollama serve in background\n",
    "        process = subprocess.Popen(['ollama', 'serve'], \n",
    "                                 stdout=subprocess.PIPE, \n",
    "                                 stderr=subprocess.PIPE,\n",
    "                                 preexec_fn=os.setsid if os.name != 'nt' else None)\n",
    "        \n",
    "        # Give it a moment to start\n",
    "        time.sleep(10)\n",
    "        \n",
    "        print(\"Ollama started in background\")\n",
    "        return process\n",
    "    except Exception as e:\n",
    "        print(f\"Error starting Ollama: {e}\")\n",
    "        return None\n",
    "\n",
    "if 'google.colab' in sys.modules or 'CI' in os.environ:\n",
    "    # Clean up any existing Ollama processes\n",
    "    kill_ollama()\n",
    "    # Start Ollama\n",
    "    ollama_process = start_ollama()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8019eb70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ollama version is 0.9.6\n"
     ]
    }
   ],
   "source": [
    "!ollama --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577af69b",
   "metadata": {},
   "source": [
    "## Chargement du Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "359e435a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "84e00389",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ListResponse(models=[])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b2a6581",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProgressResponse(status='success', completed=None, total=None, digest=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pull the Gemma3n model\n",
    "# https://ollama.com/search\n",
    "ollama.pull('gemma3n:latest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b84170bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ListResponse(models=[Model(model='gemma3n:latest', modified_at=datetime.datetime(2025, 7, 27, 13, 6, 38, 43613, tzinfo=TzInfo(+02:00)), digest='15cb39fd9394fd2549f6df9081cfc84dd134ecf2c9c5be911e5629920489ac32', size=7547589116, details=ModelDetails(parent_model='', format='gguf', family='gemma3n', families=['gemma3n'], parameter_size='6.9B', quantization_level='Q4_K_M'))])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00ba48c",
   "metadata": {},
   "source": [
    "## Interaction avec le Modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a19bb166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La France a gagné la Coupe du Monde de football pour la première fois en **1998**. \n",
      "\n",
      "Ils ont remporté le tournoi qui s'est déroulé en France, en battant l'Italie en finale sur le score de 1-0.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "class Client:\n",
    "    def __init__(self, model=\"gemma3n:latest\"):\n",
    "        self.client = ollama.AsyncClient()\n",
    "        self.model = model\n",
    "        self.messages = []\n",
    "\n",
    "    async def chat(self, content: str):\n",
    "        self.messages.append({\"role\": \"user\", \"content\": content})\n",
    "        response = await self.client.chat(self.model, messages=self.messages)\n",
    "        msg = response.message.model_dump() # pydantic\n",
    "        self.messages.append(msg)\n",
    "        return msg[\"content\"]\n",
    "\n",
    "client = Client()\n",
    "text = await client.chat(\"Quand la France a-t-elle gagné la Coupe du Monde de football pour la première fois ?\")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b72b2b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'user',\n",
      "  'content': 'Quand la France a-t-elle gagné la Coupe du Monde de football '\n",
      "             'pour la première fois ?'},\n",
      " {'role': 'assistant',\n",
      "  'content': 'La France a gagné la Coupe du Monde de football pour la première '\n",
      "             'fois en **1998**. \\n'\n",
      "             '\\n'\n",
      "             \"Ils ont remporté le tournoi qui s'est déroulé en France, en \"\n",
      "             \"battant l'Italie en finale sur le score de 1-0.\\n\",\n",
      "  'thinking': None,\n",
      "  'images': None,\n",
      "  'tool_calls': None}]\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pprint.pp(client.messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd55c5d1",
   "metadata": {},
   "source": [
    "## Ménage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b6cffbbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ListResponse(models=[])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama.delete('gemma3n:latest')\n",
    "ollama.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54fc15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'google.colab' in sys.modules or 'CI' in os.environ:\n",
    "    # Terminate Ollama\n",
    "    if ollama_process and ollama_process.poll() is None:\n",
    "        ollama_process.kill()\n",
    "        ollama_process.wait(timeout=5)\n",
    "        print(\"Ollama process killed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
