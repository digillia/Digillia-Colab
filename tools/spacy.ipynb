{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SpaCy\n",
    "\n",
    "[![Index](https://img.shields.io/badge/Index-blue)](../index.ipynb)\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/digillia/Digillia-Colab/blob/main/tools/spacy.ipynb)\n",
    "\n",
    "Comme [NLTK](./nltk.ipynb), SpaCy est une bibliothèque de code pour le traitement du langage.  \n",
    "\n",
    "Docs:\n",
    "- https://spacy.io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "\n",
    "# Supprimer les commentaires pour installer\n",
    "# !pip3 install -q -U scikit-learn\n",
    "\n",
    "# À installer dans tous les cas pour Google Colab\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    !pip3 install -q -U spacy\n",
    "    !python3 -m spacy download fr_core_news_sm # modèle compact pour le français"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "text = \"la petite chienne court après la balle.\"\n",
    "nlp = spacy.load('fr_core_news_sm')\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenisation\n",
    "\n",
    "La tokenisation consiste à découper un texte en unités plus petites, appelées tokens, qui peuvent ensuite être traitées par des modèles d'apprentissage automatique de traitement du langage naturel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "la\n",
      "petite\n",
      "chienne\n",
      "court\n",
      "après\n",
      "la\n",
      "balle\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatisation\n",
    "\n",
    "La lemmatisation consiste à réduire un mot à sa forme de base, appelée “lemme”. Le lemme est généralement un mot du dictionnaire qui représente le mot d'origine. Par exemple, le lemme de l'adjectif “petite” est “petit”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "la -> le\n",
      "petite -> petit\n",
      "chienne -> chienne\n",
      "court -> court\n",
      "après -> après\n",
      "la -> le\n",
      "balle -> balle\n",
      ". -> .\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, '->', token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PoS Tagging\n",
    "\n",
    "Le Part-of-Speech Tagging ou l'étiquetage grammatical (en français) essaye d’attribuer une étiquette à chaque mot d’une phrase représentant la classe grammaticale de ce mot (nom propre, adjectif, verbe, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "la -> DET\n",
      "petite -> ADJ\n",
      "chienne -> NOUN\n",
      "court -> ADJ\n",
      "après -> ADP\n",
      "la -> DET\n",
      "balle -> NOUN\n",
      ". -> PUNCT\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, '->', token.pos_) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop words\n",
    "\n",
    "Les stop words sont un ensemble de mots couramment utilisés qui n’apportent pas d’information pour comprendre le sens texte, comme le, la, un, une, etc. En traitement du langage natural, on cherche souvent à les éliminer pour se concentrer sur les mots riches de sens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[petite, chienne, court, balle, .]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.lang.fr.stop_words import STOP_WORDS # contient les stop words en français\n",
    "\n",
    "meaningful_tokens = []\n",
    "for token in doc:\n",
    "    if token.text not in STOP_WORDS:\n",
    "        meaningful_tokens.append(token)\n",
    "meaningful_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embedding\n",
    "\n",
    "Le Word Embedding (ou plongement lexical en français) est une méthode d'encodage qui vise à représenter les mots ou les phrases d’un texte par des vecteurs de nombres réels, décrit dans un modèle vectoriel (ou Vector Space Model), notamment à des fins de comparaison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "petite -> [-1.1801080e+00  4.4308090e+00  1.6367952e+00 -3.2228432e+00\n",
      " -2.4488101e+00 -3.0054703e+00  2.7773752e+00  1.1064811e+00\n",
      " -5.8037248e+00  2.6085579e+00 -1.4868605e+00  4.3556032e+00\n",
      " -8.3231008e-01  3.6200447e+00  2.9481339e-01 -4.5109671e-01\n",
      "  6.9591045e+00  2.0072207e+00 -7.6686025e-02 -3.9068332e+00\n",
      " -7.6405029e+00 -1.0756768e+00 -1.8130282e-01 -3.0047803e+00\n",
      "  6.0534353e+00 -1.8673252e+00 -6.0592431e-01 -7.5262403e-01\n",
      " -1.7681978e+00 -1.1085639e+00 -2.5105045e+00 -3.0608320e+00\n",
      "  2.0636497e+00  3.5261813e-01 -5.6750250e-01 -6.3179249e-01\n",
      " -1.9752696e+00 -2.2256575e+00 -2.8794732e+00 -6.4547195e+00\n",
      " -1.7097976e+00 -3.4736013e+00  7.1012270e-01 -3.4470658e+00\n",
      "  7.3811941e+00 -3.5134351e+00 -4.9389844e+00 -4.2680292e+00\n",
      " -1.2265478e+00 -3.2707798e+00 -9.9603039e-01  1.9791358e+00\n",
      "  3.3294590e+00 -8.4403181e-01 -3.4321225e-01  4.1279154e+00\n",
      " -3.3908823e+00  1.3627882e+00  6.4954245e-01 -3.8389239e+00\n",
      " -8.1345105e-01  3.5860665e+00 -1.0751721e+00 -3.7765405e+00\n",
      "  1.5234199e+00  1.8057418e+00  6.6162825e+00  1.4444001e+00\n",
      "  3.3357618e+00  2.1022730e+00 -1.9999835e+00 -4.3153410e+00\n",
      "  7.7524376e-01  1.3000178e+01 -1.2136489e-02  5.0061994e+00\n",
      "  3.8688517e+00 -3.5922990e+00 -1.3389132e+00 -8.1481676e+00\n",
      "  1.4444834e+00  1.3859601e+00  4.9604970e-01  3.4194841e+00\n",
      "  3.5619822e+00  5.2768636e-01  2.6533122e+00  8.4650898e+00\n",
      " -5.6155243e+00 -1.5608554e+00 -3.3346097e+00  6.7861309e+00\n",
      " -2.1214433e+00  6.4054203e+00 -7.8934503e-01  2.7292943e-01]\n",
      "chienne -> [-1.6806116   1.3095278   4.4752564   1.9814914   0.4789462   3.9858258\n",
      " -3.301079    6.7044907  -4.504014   -2.7889376  -0.41827634 -3.0004928\n",
      " -2.1792717   2.2358904   1.552773    5.7349124  -0.6294889  -0.7123136\n",
      " -2.106407    1.8953611  -2.820686    0.5167624  -1.7455858  -1.4831454\n",
      " -0.8212155  -3.5218074   0.9551438   0.5589634   2.1479554  -6.556424\n",
      " -0.02281654 -1.6288851   0.20556292 -2.9618728   3.595803    4.7669096\n",
      " -4.036504   -2.799724    0.42275167 -1.5001767  -1.6411898   5.2874455\n",
      " -2.7348547   5.0826077  -1.9424249   2.257387   -0.24852455 -4.4352684\n",
      " -2.8342235   0.17963988 -2.9598413  -3.4813218   1.9882005  -6.8350935\n",
      "  8.8817425   1.0255375  -6.0437775   5.073482   -2.6475246   0.9300463\n",
      "  2.1100776   2.323038   -0.6898516  -1.3671223   6.4492497   1.7083676\n",
      "  2.6639557   1.4411219  -1.4634706  -2.4762769  -2.7647724   1.8069388\n",
      "  1.3939863   1.4577312   0.6269797   3.9366198  -1.0903759  -2.1309724\n",
      " -4.1901317  -5.3413196  -2.1824107   0.742483   -1.5966367  -3.9841623\n",
      "  1.7395978  10.535305   -1.1028831   0.48022723  4.549646   -0.41970336\n",
      " -3.9021177  -4.209407    6.8692093   2.6781049  -1.6010599  -0.88838166]\n",
      "court -> [ 3.6632326   0.34073946 -3.804978    1.1267064   3.0385547   4.6908636\n",
      "  0.08741128 -1.8788369   2.1125655  -0.9602294  -1.1158198   2.672328\n",
      "  0.18443286 -0.85726166  2.1670227   3.666926    1.9444393  -0.99947107\n",
      "  2.820586   -0.9936466   1.4875844  -2.499476   -3.4929714   3.2206612\n",
      " -2.509266    8.65345     1.7687364  -2.2028396  -1.2959524   0.10144764\n",
      " -1.7965804  -4.0414224  -4.8253565   4.893343    1.5515972  -5.692584\n",
      " -0.9365352  -0.00957257 -0.884089   -6.1285305  -0.26754868  1.1580286\n",
      " -1.7654543  -2.1497123   0.84021074 -4.9999332  -1.1123794  -2.985983\n",
      "  0.91361535  0.28220356  1.8166037  -4.758856   -5.3282633   6.739949\n",
      " -2.6313863   1.1101027  -2.8684063  -3.9227476  -2.687306   -1.6609774\n",
      "  4.3774548   1.2159655   2.0538692   2.8656223   0.9440721   2.240261\n",
      " -2.1491396   0.8232597   3.259023    2.8516057  -3.045504    0.4538591\n",
      " -0.36689752 -7.244214   -0.9680858   7.0148153   1.6518612   1.5432737\n",
      " -2.938005   -0.8620044  -5.3477764  -1.6577152   1.2211912   5.017414\n",
      "  2.815143    5.480263   -2.0256433   6.374588   -1.8371506  -4.800197\n",
      "  2.1347442   3.8200614  -1.2116325   4.4124465  -2.516902   -3.5402188 ]\n",
      "balle -> [-2.7501264e+00 -1.8255122e+00  3.8332267e+00  2.0292416e-01\n",
      " -1.2961620e-01  2.2578576e+00  3.4393921e+00  2.4751410e+00\n",
      " -4.4620285e+00 -2.1124732e+00 -2.5819027e+00 -6.5443473e+00\n",
      " -4.6478348e+00  2.0419588e+00 -2.4013324e+00  3.2168391e+00\n",
      "  2.2673390e+00 -2.1361589e+00 -3.9863281e+00  3.3095980e+00\n",
      " -8.4579496e+00  3.7662313e+00 -1.8663290e-01 -2.9185309e+00\n",
      "  2.1304402e-01 -1.8921282e+00  7.6904035e-01  4.8211747e-01\n",
      " -4.3920417e+00 -4.4577551e+00 -7.0967078e-01  1.5815334e+00\n",
      " -1.8334110e+00 -2.2376297e+00  4.8520470e+00  3.5185325e+00\n",
      "  7.9573512e-02  3.3709476e+00 -7.4784875e-02 -1.7418107e+00\n",
      "  2.4590924e+00  4.7729616e+00 -2.7989239e-02 -1.1326331e-01\n",
      " -3.0559969e+00  3.8023934e-01 -3.1784549e+00 -4.6976075e+00\n",
      " -1.3353577e+00  9.9745989e-02 -9.7516000e-01  6.3069594e-01\n",
      "  7.5965762e-01 -4.4623280e+00  5.7593880e+00 -1.4728868e+00\n",
      " -1.3684250e+00  2.8439517e+00  3.1990137e+00  6.4091426e-01\n",
      " -5.5916595e-01 -2.7985904e+00 -3.8319445e+00 -1.3564599e+00\n",
      "  7.5632510e+00  1.6466228e+00  5.2501698e+00  1.6047132e+00\n",
      "  1.9690094e+00  2.2766085e+00  5.0572175e-01  2.6216760e+00\n",
      " -8.2248533e-01 -8.6252940e-01  5.9350414e+00 -3.7916580e-01\n",
      "  3.7228620e-01 -2.1338313e+00  2.6317999e+00 -1.0209253e+00\n",
      "  9.9582112e-01  3.2253118e+00 -6.7109168e-03 -2.2907457e+00\n",
      " -1.1523845e+00  5.7481594e+00 -1.5327340e+00  3.7932806e+00\n",
      " -1.4032121e+00  4.5864296e+00 -3.9563253e+00 -8.9875877e-02\n",
      " -2.8328364e+00  4.1325569e-01 -1.9005100e+00 -3.7546654e+00]\n",
      ". -> [-1.9040627  -1.7671411   0.02890182 -1.9408572  -0.10894525  3.2676728\n",
      " -0.6877229  -3.7661405   1.2520008   0.13387495 -0.33297157 -2.5753427\n",
      " -2.6064415   5.859246   -2.9187078  -1.9292555   0.9079473  -4.8242426\n",
      " -2.5273378  -2.6421824   6.909292    3.2513375   0.7275791   2.1691203\n",
      " -2.4436796   1.954454    5.4723787   0.05980194 -0.9043422  -2.0190706\n",
      "  7.544621   -4.704433    0.7946946   1.5091553   2.265397   -1.067309\n",
      " -0.13517833  2.4684772  -1.1332983   4.5025687   5.3402834  -1.3967258\n",
      " -1.5215752   6.9832134   1.5547345  -4.0216126   7.659118    4.896659\n",
      " -3.1068974  -0.76092887  3.7029629   3.2514648   5.920941    1.5984209\n",
      "  3.670023   -1.9013608   0.06991142  1.7370968  -3.5239882   0.3451326\n",
      " -2.1209888  -0.22754502 -5.7207155  -0.60792595 -3.8824966   0.48749763\n",
      " -3.1541123  -1.3272197  -0.2771685  -4.308682    3.817187   -0.9585359\n",
      " -0.46006733 -2.737074   -2.809178   -6.3777943  -4.389758    7.780128\n",
      "  1.2773662   3.3836195  -3.3499093  -2.9061704  -6.4050097  -1.3053846\n",
      " -0.15716487  2.816144    3.2466698   7.364496   -5.9090567   0.35669672\n",
      " -1.8088723  -4.2411575   0.8695807  -0.7795845  -1.6712333  -3.2096334 ]\n"
     ]
    }
   ],
   "source": [
    "for token in meaningful_tokens:\n",
    "    print(token.text, '->', token.vector) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarités\n",
    "\n",
    "La représentatin en vecteurs permet de mesurer la similarité par cosinus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chien chat 0.41185740332426884\n",
      "voiture train 0.2718245108507194\n",
      "chien voiture -0.009769492522194518\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "doc1 = nlp('chien')\n",
    "doc2 = nlp('chat')\n",
    "doc3 = nlp('voiture')\n",
    "doc4 = nlp('train')\n",
    "\n",
    "print(doc1, doc2, doc1.similarity(doc2))\n",
    "print(doc3, doc4, doc3.similarity(doc4))\n",
    "print(doc1, doc3, doc1.similarity(doc3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
