{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Gemini (and Gemma)\n",
    "\n",
    "[![Index](https://img.shields.io/badge/Index-blue)](../index.ipynb)\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/digillia/Digillia-Colab/blob/main/tools/gemini.ipynb)\n",
    "\n",
    "Ce bloc-note Jupyter explore brièvement les capacités de [Gemini](https://aistudio.google.com/) à travers son API en langage Python. D'une manière générale, on préférera utiliser les modèles de Google à travers les API de [LangChain](./langchain.ipynb) ou [LlamaIndex](./llamaindex.ipynb) pour les applications de génération augmentée de récupération (RAG).\n",
    "\n",
    "Docs:\n",
    "- https://ai.google.dev/\n",
    "- https://github.com/google/generative-ai-python\n",
    "- https://github.com/google/generative-ai-docs/tree/main/site/en/gemini-api/tutorials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Supprimer les commentaires pour installer (requirements.txt)\n",
    "# !pip3 install -qU python-dotenv\n",
    "# !pip3 install -qU jax\n",
    "\n",
    "# À installer dans tous les cas pour Google Colab et Github\n",
    "if ('google.colab' in sys.modules) or ('CI' in os.environ):\n",
    "    !pip3 install -qU google-generativeai\n",
    "    !pip3 install -qU huggingface_hub\n",
    "    !pip3 install -qU git+https://github.com/google-deepmind/gemma.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement de la Clé pour Gemini\n",
    "\n",
    "Il vous faut [obtenir de Google une clé](https://aistudio.google.com/app/prompts/new_chat/?hl=fr) pour exécuter ce bloc-note Jupyter. Ensuite, le chargement se fait soit à partir de l'environnement (fichier `.env`), soit à partir des secrets de Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_api_key = None\n",
    "if 'google.colab' in sys.modules:\n",
    "  from google.colab import userdata\n",
    "  google_api_key = userdata.get('GOOGLE_API_KEY')\n",
    "  huggingface_api_key = userdata.get('HUGGINGFACE_API_KEY')\n",
    "else:\n",
    "  from dotenv import load_dotenv, find_dotenv\n",
    "  _ = load_dotenv(find_dotenv()) # lire le fichier .env local\n",
    "  google_api_key  = os.environ['GOOGLE_API_KEY']\n",
    "  huggingface_api_key = os.environ['HUGGINGFACE_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "genai.configure(api_key=google_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choix de modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/chat-bison-001\n",
      "models/text-bison-001\n",
      "models/embedding-gecko-001\n",
      "models/gemini-1.0-pro-latest\n",
      "models/gemini-1.0-pro\n",
      "models/gemini-pro\n",
      "models/gemini-1.0-pro-001\n",
      "models/gemini-1.0-pro-vision-latest\n",
      "models/gemini-pro-vision\n",
      "models/gemini-1.5-pro-latest\n",
      "models/gemini-1.5-pro-001\n",
      "models/gemini-1.5-pro-002\n",
      "models/gemini-1.5-pro\n",
      "models/gemini-1.5-pro-exp-0801\n",
      "models/gemini-1.5-pro-exp-0827\n",
      "models/gemini-1.5-flash-latest\n",
      "models/gemini-1.5-flash-001\n",
      "models/gemini-1.5-flash-001-tuning\n",
      "models/gemini-1.5-flash\n",
      "models/gemini-1.5-flash-exp-0827\n",
      "models/gemini-1.5-flash-002\n",
      "models/gemini-1.5-flash-8b\n",
      "models/gemini-1.5-flash-8b-001\n",
      "models/gemini-1.5-flash-8b-latest\n",
      "models/gemini-1.5-flash-8b-exp-0827\n",
      "models/gemini-1.5-flash-8b-exp-0924\n",
      "models/gemini-2.0-flash-exp\n",
      "models/gemini-exp-1206\n",
      "models/gemini-exp-1121\n",
      "models/gemini-exp-1114\n",
      "models/learnlm-1.5-pro-experimental\n",
      "models/embedding-001\n",
      "models/text-embedding-004\n",
      "models/aqa\n"
     ]
    }
   ],
   "source": [
    "for model in genai.list_models():\n",
    "    print(model.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = genai.GenerativeModel(\n",
    "  #model_name='gemini-1.5-flash',\n",
    "  #model_name='gemini-1.5-flash-8b',\n",
    "  #model_name='gemini-1.5-pro',\n",
    "  model_name='gemini-2.0-flash-exp',\n",
    "  generation_config={\n",
    "    'temperature': 1,\n",
    "    'top_p': 0.95,\n",
    "    'top_k': 40,\n",
    "    'max_output_tokens': 8192,\n",
    "    'response_mime_type': 'text/plain',\n",
    "  },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation de texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Napoléon avait **35 ans** lorsqu'il a été couronné empereur des Français le 2 décembre 1804. Il était né le 15 août 1769.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "from IPython.display import Markdown\n",
    "\n",
    "response = model.generate_content(\"Quel age avait Napoléon quand il a été couronné empereur?\")\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversation Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "La Première Guerre mondiale, aussi appelée la Guerre de 14-18, s'est terminée le **11 novembre 1918** avec la signature de l'armistice.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "from IPython.display import Markdown\n",
    "\n",
    "chat_session = model.start_chat(history=[])\n",
    "response = chat_session.send_message('Quand la guerre de 14-18 s\\'est-elle terminée?')\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "La Première Guerre mondiale s'est terminée par la signature de l'armistice dans un wagon de train, dans la **clairière de Rethondes**, en forêt de Compiègne, dans le nord de la France.\n",
       "\n",
       "Il est important de noter que l'armistice n'est pas la fin officielle de la guerre. Les traités de paix, notamment le Traité de Versailles, ont été signés plus tard en 1919 pour officialiser la fin du conflit. Cependant, l'armistice du 11 novembre 1918 marque la fin des combats.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = chat_session.send_message('Où s\\'est-elle terminée?')\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[parts {\n",
       "   text: \"Quand la guerre de 14-18 s\\'est-elle terminée?\"\n",
       " }\n",
       " role: \"user\",\n",
       " parts {\n",
       "   text: \"La Première Guerre mondiale, aussi appelée la Guerre de 14-18, s\\'est terminée le **11 novembre 1918** avec la signature de l\\'armistice.\\n\"\n",
       " }\n",
       " role: \"model\",\n",
       " parts {\n",
       "   text: \"Où s\\'est-elle terminée?\"\n",
       " }\n",
       " role: \"user\",\n",
       " parts {\n",
       "   text: \"La Première Guerre mondiale s\\'est terminée par la signature de l\\'armistice dans un wagon de train, dans la **clairière de Rethondes**, en forêt de Compiègne, dans le nord de la France.\\n\\nIl est important de noter que l\\'armistice n\\'est pas la fin officielle de la guerre. Les traités de paix, notamment le Traité de Versailles, ont été signés plus tard en 1919 pour officialiser la fin du conflit. Cependant, l\\'armistice du 11 novembre 1918 marque la fin des combats.\\n\"\n",
       " }\n",
       " role: \"model\"]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_session.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gemma\n",
    "\n",
    "Pour accéder au modèle Gemma sur Hugging Face, il faut:\n",
    "- Une clé Hugging Face obtenue sur https://huggingface.co/settings/tokens\n",
    "- Accepter l'accord de licence de Gemma sur https://huggingface.co/google/gemma-2-2b-it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f247ced50a24e7f874d0ec9167cd0d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 15 files:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "local_dir = snapshot_download(repo_id=\"google/gemma-2b-flax\", token=huggingface_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gemma import params as params_lib\n",
    "from gemma import transformer as transformer_lib\n",
    "import sentencepiece as spm\n",
    "\n",
    "vocab = spm.SentencePieceProcessor()\n",
    "vocab.Load(f'{local_dir}/tokenizer.model')\n",
    "params = params_lib.load_and_format_params(f'{local_dir}/2b')\n",
    "transformer_config = transformer_lib.TransformerConfig.from_params(\n",
    "    params=params,\n",
    "    cache_size=1024\n",
    ")\n",
    "transformer = transformer_lib.Transformer(transformer_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n\\nAnswer:\\n\\nThe first world war lasted from\\n1914 to 1918.']\n"
     ]
    }
   ],
   "source": [
    "from gemma import sampler as sampler_lib\n",
    "\n",
    "sampler = sampler_lib.Sampler(\n",
    "    transformer=transformer,\n",
    "    vocab=vocab,\n",
    "    params=params['transformer'],\n",
    ")\n",
    "# Gemma 2b comprend mal le français\n",
    "prompt = ['How long lasted the first world war?']\n",
    "response = sampler(input_strings=prompt, total_generation_steps=128)\n",
    "print(response.text)\n",
    "# Supprimer le sampler pour un nouveau prompt\n",
    "del sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour supprimer les modèles du cache\n",
    "# !pip3 install -q -U \"huggingface_hub[cli]\"\n",
    "# !huggingface-cli scan-cache\n",
    "# !huggingface-cli delete-cache"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
