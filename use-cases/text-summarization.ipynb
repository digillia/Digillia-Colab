{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Résumé de Textes\n",
    "\n",
    "Nous démontrons ici différentes méthodes simples pour résumer des documents en utilisant des modèles de langage larges.\n",
    "\n",
    "[![Index](https://img.shields.io/badge/Index-blue)](../index.ipynb)\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/digillia/Digillia-Colab/blob/main/use-cases/text-summarization.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Supprimer les commentaires pour installer (requirements.txt)\n",
    "# !pip3 install -q -U ipywidgets\n",
    "# !pip3 install -q -U python-dotenv\n",
    "# !pip3 install -q -U torch\n",
    "# !pip3 install -q -U tqdm\n",
    "# !pip3 install -q -U transformers\n",
    "\n",
    "# À installer dans tous les cas pour Google Colab et Github\n",
    "if 'google.colab' in sys.modules or 'CI' in os.environ:\n",
    "  # BEGIN\n",
    "  # llmx cause de multiples problèmes de dépendances avec les librairies installées ensuite\n",
    "  # see https://community.openai.com/t/error-while-importing-openai-from-open-import-openai/578166\n",
    "  # see https://stackoverflow.com/questions/77759146/issue-installing-openai-in-colab\n",
    "  !pip3 uninstall llmx -y\n",
    "  # END\n",
    "  !pip3 install -q -U langchain\n",
    "  !pip3 install -q -U langchain-openai\n",
    "  !pip3 install -q -U llama-index\n",
    "  !pip3 install -q -U openai\n",
    "  !pip3 install -q -U pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cette variable python est accessible depuis les commandes bash\n",
    "work_directory = \"text-summarization\"\n",
    "\n",
    "# Récupération des données pour Google Colab et Github\n",
    "if 'google.colab' in sys.modules or 'CI' in os.environ:\n",
    "  !curl --create-dirs -O --output-dir $work_directory \"https://raw.githubusercontent.com/digillia/Digillia-Colab/main/use-cases/text-summarization/sample_1.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction utilitaire pour afficher du texte sur plusieurs lignes\n",
    "def word_wrap(string, n_chars=72):\n",
    "    if len(string) < n_chars:\n",
    "        return string\n",
    "    else:\n",
    "        return string[:n_chars].rsplit(' ', 1)[0] + '\\n' + word_wrap(string[len(string[:n_chars].rsplit(' ', 1)[0])+1:], n_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des textes à résumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      " Karan Singh, Assistant Professor of Operations Research \n",
      "\n",
      "Characteristics common to both language models and supervised learning:\n",
      "1.Predicting Well is the Yardstick. A prediction rule is good as long\n",
      "as it makes reasonable predictions on average. Compared to more\n",
      "ambitious sub-disciplines in statistics, any statements about\n",
      "causality, p-values, and recovering latent structure are absent. We are\n",
      "similarly impervious to such considerations in language models. Such\n",
      "simplicity of goals enables very flexible prediction rules in machine\n",
      "learning. Although seeming modest in its aim, the art of machine\n",
      "learning has long been to cast as many disparate problems as questions\n",
      "about prediction as possible. Predicting house prices from square\n",
      "footage is a regular regression task. But, for reverse image\n",
      "captioning, is “predicting” a (high-dimensional) image given a few\n",
      "words a reasonable or well-defined classification task? Yet, this is\n",
      "how machine learning algorithms function. 2.Model Agnosticism.\n",
      "Supervised learning algorithms realize the adage that all models are\n",
      "wrong, but some are useful. For example, when building the price\n",
      "predictor above, a data scientist does not believe that the genuine\n",
      "relationship between prices and area is linear or well-specified.\n",
      "Similarly, when using neural networks to predict the next word in\n",
      "language models, we don’t believe that this is how Shakespeare must\n",
      "have employed a neural network to compose his texts. Yet, there are\n",
      "crucial differences:  5\n",
      "Figure 5: Predicting house prices from square\n",
      "footage. Pictured is a linear regression, an example of a supervised\n",
      "learning algorithm that uses extant data to learn a linear predictor.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pypdf import PdfReader\n",
    "\n",
    "# Empty list to store page text\n",
    "texts = []\n",
    "\n",
    "# Loop through each PDF file\n",
    "for file_name in os.listdir(work_directory):\n",
    "    \n",
    "    # Check if file is a PDF\n",
    "    if file_name.endswith('.pdf'):\n",
    "        \n",
    "        # Create PDF file object\n",
    "        reader = PdfReader(os.path.join(work_directory, file_name))\n",
    "        \n",
    "        # Loop through pages and extract text\n",
    "        for page in reader.pages:\n",
    "            \n",
    "            # Extract text from page\n",
    "            texts.append(page.extract_text())\n",
    "            \n",
    "print(len(texts))\n",
    "print(word_wrap(texts[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement de la Clé pour OpenAI\n",
    "\n",
    "Il vous faut obtenir d'Open AI une clé pour exécuter ce notebook Jupyter. Vous pouvez consulter [Where do I find my API key?](https://help.openai.com/en/articles/4936850-where-do-i-find-my-api-key). Ensuite, le chargement se fait soit à partir de l'environnement (fichier `.env`), soit à partir des secrets de Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "openai_api_key = None\n",
    "if 'google.colab' in sys.modules:\n",
    "  from google.colab import userdata\n",
    "  openai_api_key = userdata.get('OPENAI_API_KEY')\n",
    "else:\n",
    "  from dotenv import load_dotenv, find_dotenv\n",
    "  _ = load_dotenv(find_dotenv()) # read local .env file\n",
    "  openai_api_key  = os.environ['OPENAI_API_KEY']\n",
    "openai.api_key = openai_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Résumé de Texte avec LlamaIndex\n",
    "\n",
    "Cette approche utilise l'API Restful d'Open AI par le framework LlamaIndex \n",
    "\n",
    "Docs: https://docs.llamaindex.ai/en/stable/examples/index_structs/doc_summary/DocSummary.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb0d42994f5a44d8bc22c6b8d4ff7810",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a727d2a18f9e4fe297fa981fca97f980",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarizing documents:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 80e21738-60e9-4c5b-9f62-c1ad031eadbb\n",
      "current doc id: e7fe4290-efef-4045-b767-994e58f2e559\n",
      "current doc id: 5155facd-f403-4e4d-94b1-6f1f23aaa0c9\n",
      "current doc id: 7b5096a7-22d7-4eec-9157-ccae18871a8d\n",
      "current doc id: 384a58bb-6d1c-49f0-8b66-341c67244a07\n",
      "current doc id: a8c8647d-dfbf-4d9d-9b66-b5f5cefde09a\n",
      "current doc id: fd845f07-eb80-4950-aff9-99b46b990e99\n",
      "current doc id: f9128ffb-3838-4b03-bb10-a44475917961\n",
      "current doc id: 6047c5a5-7be7-4f20-860e-952e72f4dc4a\n",
      "current doc id: 90a4724c-f857-4b4f-8128-7b8b300f7669\n",
      "current doc id: 00f9b1b1-24e8-4da4-bd48-02e9f6289d2f\n",
      "current doc id: e244c9a1-7980-4b12-883f-236196bdf94f\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0548087b35fb4998ab335fc2c1b091da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text discusses the challenges of using large language models for\n",
      "supervised learning tasks due to computational constraints. It\n",
      "introduces in-context learning as a more cost-effective approach to\n",
      "repurpose pre-trained language models for specific tasks by providing\n",
      "labeled examples in the prompt. This method eliminates the need for\n",
      "computationally expensive adjustments to the model's parameters, making\n",
      "it more user-friendly. While fine-tuning may offer performance gains,\n",
      "in-context learning reduces the computational load, making large\n",
      "language models more accessible for end-users. The text also mentions\n",
      "GPT3's advancement in narrowing the performance gap between fine-tuning\n",
      "and in-context learning, thereby democratizing the use of large\n",
      "language models.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core import (\n",
    "  SimpleDirectoryReader,\n",
    "  get_response_synthesizer,\n",
    ")\n",
    "from llama_index.core.indices import DocumentSummaryIndex\n",
    "\n",
    "documents = SimpleDirectoryReader(work_directory).load_data()\n",
    "llm = OpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
    "embedding_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
    "response_synthesizer = get_response_synthesizer(response_mode=\"tree_summarize\", use_async=True)\n",
    "index = DocumentSummaryIndex.from_documents(\n",
    "    documents,\n",
    "    llm=llm,\n",
    "    embedding_model=embedding_model,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    show_progress=True,\n",
    ")\n",
    "query_engine = index.as_query_engine()\n",
    "query = query_engine.query(\"Could you summarize the given context? Return your response which covers the key points of the text and does not miss anything important, please.\")\n",
    "print(word_wrap(query.response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Résumé de Texte avec LangChain\n",
    "\n",
    "Cette approche utilise l'API Restful d'Open AI par le framework LangChain \n",
    "\n",
    "Docs: https://python.langchain.com/docs/use_cases/summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "documents = PyPDFDirectoryLoader(work_directory).load()\n",
    "llm_name = \"gpt-3.5-turbo\"\n",
    "llm = ChatOpenAI(model_name=llm_name, openai_api_key=openai_api_key, temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L'Approche Prompt (se heurte à la taille du context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from openai import BadRequestError\n",
    "\n",
    "def format_docs(documents):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "\n",
    "template = \"Summarize the main themes in these retrieved docs: {docs}\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "chain = {\"docs\": format_docs} | prompt | llm | StrOutputParser()\n",
    "try:\n",
    "    chain.invoke(documents)\n",
    "except BadRequestError as e:\n",
    "    print(e.body['message'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L'Approche MapReduce\n",
    "\n",
    "Il s'agit de résumer chacun des documents, puis de résumer l'ensemble des résumés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generative artificial intelligence (GenAI) tools are advanced\n",
      "algorithms that can create new content based on user prompts in various\n",
      "formats, reaching human-level performance and offering potential\n",
      "benefits for businesses. The report by Karan Singh explores new-era AI\n",
      "technologies, such as large language models (LLMs), and their impact on\n",
      "tasks and future advances. It discusses the importance of initial\n",
      "prompts in language models, supervised learning techniques, and the\n",
      "evolution of neural network architectures. The paper also highlights\n",
      "the limitations of models like GPT3 and introduces new models like\n",
      "InstructGPT and foundation models. Tradeoffs in models like GPT4 are\n",
      "discussed, along with methods for improving text generation.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "chain = load_summarize_chain(llm, chain_type=\"map_reduce\", token_max=3000)\n",
    "response = chain.invoke(documents)\n",
    "print(word_wrap(response['output_text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L'Approche Refine\n",
    "\n",
    "Il s'agit de résumer chaque document avec le résumé des précédents, jusqu'à atteindre le dernier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The existing summary provides a comprehensive overview of Generative\n",
      "artificial intelligence (GenAI) tools, particularly large language\n",
      "models (LLMs), and their impact on various business processes.\n",
      "Assistant Professor Karan Singh's report delves into the functioning\n",
      "and principles of GenAI, focusing on LLMs and their successful\n",
      "applications in commercial settings. The report also discusses the\n",
      "challenges and advancements in supervised learning, deep learning, and\n",
      "the evolution of language processing tasks using word embeddings,\n",
      "recurrent neural networks, and transformers. It highlights the\n",
      "practical applications of large language models, specifically\n",
      "in-context learning, as a cost-effective and efficient way to repurpose\n",
      "pre-trained models for specific tasks. The report also touches on the\n",
      "future potential of foundation models to expand the use of LLMs in\n",
      "other domains. The report also provides insights into the tradeoffs\n",
      "between fine-tuning and in-context learning, as well as the performance\n",
      "of GPT4 on various exams and the concerns raised by industry and\n",
      "research leaders. Overall, the report offers valuable insights into the\n",
      "current landscape of GenAI and its implications for various industries.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "chain = load_summarize_chain(llm, chain_type=\"refine\")\n",
    "response = chain.invoke(documents)\n",
    "print(word_wrap(response['output_text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Résumé de Texte avec Google T5\n",
    "\n",
    "Cette approche utilise directement un modèle de langage large en local.\n",
    "\n",
    "Docs:\n",
    "- https://huggingface.co/docs/transformers/tasks/summarization\n",
    "- https://huggingface.co/docs/transformers/model_doc/t5\n",
    "- https://www.analyticsvidhya.com/blog/2023/06/pdf-summarization-with-transformers-in-python/\n",
    "- https://blog.research.google/2022/03/auto-generated-summaries-in-google-docs.html\n",
    "- https://blog.research.google/2020/06/pegasus-state-of-art-model-for.html\n",
    "- https://medium.com/gopenai/text-summarization-using-flan-t5-5ded2e4ce182\n",
    "- https://medium.com/artificialis/t5-for-text-summarization-in-7-lines-of-code-b665c9e40771"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour supprimer le modèle du cache\n",
    "# !pip3 install -q -U \"huggingface_hub[cli]\"\n",
    "# !huggingface-cli delete-cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Initialisation du Modèle et de son Tokenizer\n",
    "\n",
    "Notez que les modèles T5-base ont environ 220 millions de paramètres, à comparer avec les environ 175 milliards de paramètres de GPT 3.5 (Open AI). On ne peut pas en attendre la même qualité de résumés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# https://huggingface.co/docs/transformers/model_doc/t5\n",
    "tr_name = 'T5-base'\n",
    "\n",
    "# https://huggingface.co/docs/transformers/model_doc/t5v1.1\n",
    "# tr_name = 'google/t5-v1_1-base'\n",
    "\n",
    "# https://huggingface.co/docs/transformers/model_doc/flan-t5\n",
    "# tr_name = 'google/flan-t5-base'\n",
    "\n",
    "# tokenizer = T5Tokenizer.from_pretrained(tr_name)\n",
    "# model = T5ForConditionalGeneration.from_pretrained(tr_name)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(tr_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(tr_name, return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Résumé d'une variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> data science is an interdisciplinary field focused on extracting\n",
      "knowledge from typically large data sets. it incorporates skills from\n",
      "computer science, statistics, information science, mathematics, data\n",
      "visualization, information visualization, data sonification, graphic\n",
      "design, complex systems, communication and business. the field\n",
      "encompasses preparing data for analysis, formulating data science\n",
      "problems, analyzing data, developing data-driven solutions.</s>\n"
     ]
    }
   ],
   "source": [
    "text = (\"Data science is an interdisciplinary field[10] focused on extracting knowledge from typically large data sets and applying the knowledge and insights from that data to solve problems in a wide range of application domains.[11] The field encompasses preparing data for analysis, formulating data science problems, analyzing data, developing data-driven solutions, and presenting findings to inform high-level decisions in a broad range of application domains. As such, it incorporates skills from computer science, statistics, information science, mathematics, data visualization, information visualization, data sonification, data integration, graphic design, complex systems, communication and business.[12][13] Statistician Nathan Yau, drawing on Ben Fry, also links data science to human–computer interaction: users should be able to intuitively control and explore data.[14][15] In 2015, the American Statistical Association identified database management, statistics and machine learning, and distributed and parallel systems as the three emerging foundational professional communities.[16]\")\n",
    "inputs = tokenizer.encode(\"sumarize: \" + text, return_tensors='pt', max_length=512, truncation=True)\n",
    "output = model.generate(inputs, min_length=80, max_length=100)\n",
    "summary = tokenizer.decode(output[0])\n",
    "print(word_wrap(summary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Résumé de Textes par l'Approche MapReduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [08:30<00:00, 42.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> openAI’s chatGPT, a conversational web app based on a generative\n",
      "(multimodal) language model, took five days to reach one million users.\n",
      "on the business side, the number of jobs mentioning AI-related skills\n",
      "quadrupled from 2022 to 2023.\n",
      "­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­\n",
      "­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­\n",
      "­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­\n",
      "­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­\n",
      "­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­\n",
      "­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­\n",
      "­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­\n",
      "­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­\n",
      "­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­\n",
      "­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­\n",
      "­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­\n",
      "­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­\n",
      "­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "summaries = []\n",
    "\n",
    "def summarize(text):\n",
    "    inputs = tokenizer.encode(\"summarize: \" + text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    outputs = model.generate(inputs, max_new_tokens=1000, min_new_tokens=250, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    return tokenizer.decode(outputs[0])    \n",
    "\n",
    "for text in tqdm(texts):\n",
    "    summaries.append(summarize(text))\n",
    "\n",
    "summary = summarize(\"\\n\\n\".join(summaries))\n",
    "print(word_wrap(summary))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
