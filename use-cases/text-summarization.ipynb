{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Résumé de Textes\n",
    "\n",
    "Nous démontrons ici différentes méthodes simples pour résumer des documents en utilisant des modèles de langage larges.\n",
    "\n",
    "[![Index](https://img.shields.io/badge/Index-blue)](../index.ipynb)\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/digillia/Digillia-Colab/blob/main/use-cases/text-summarization.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Supprimer les commentaires pour installer (requirements.txt)\n",
    "# !pip3 install -q -U ipywidgets\n",
    "# !pip3 install -q -U python-dotenv\n",
    "# !pip3 install -q -U torch\n",
    "# !pip3 install -q -U tqdm\n",
    "# !pip3 install -q -U transformers\n",
    "\n",
    "# À installer dans tous les cas pour Google Colab et Github\n",
    "if 'google.colab' in sys.modules or 'CI' in os.environ:\n",
    "  # BEGIN\n",
    "  # llmx cause de multiples problèmes de dépendances avec les librairies installées ensuite\n",
    "  # see https://community.openai.com/t/error-while-importing-openai-from-open-import-openai/578166\n",
    "  # see https://stackoverflow.com/questions/77759146/issue-installing-openai-in-colab\n",
    "  !pip3 uninstall llmx -y\n",
    "  # END\n",
    "  !pip3 install -q -U langchain\n",
    "  !pip3 install -q -U langchain-openai\n",
    "  !pip3 install -q -U llama-index\n",
    "  !pip3 install -q -U openai\n",
    "  !pip3 install -q -U pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cette variable python est accessible depuis les commandes bash\n",
    "work_directory = \"text-summarization\"\n",
    "\n",
    "# Récupération des données pour Google Colab et Github\n",
    "if 'google.colab' in sys.modules or 'CI' in os.environ:\n",
    "  !curl --create-dirs -O --output-dir $work_directory \"https://raw.githubusercontent.com/digillia/Digillia-Colab/main/use-cases/text-summarization/sample_1.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction utilitaire pour afficher du texte sur plusieurs lignes\n",
    "def word_wrap(string, n_chars=72):\n",
    "    if len(string) < n_chars:\n",
    "        return string\n",
    "    else:\n",
    "        return string[:n_chars].rsplit(' ', 1)[0] + '\\n' + word_wrap(string[len(string[:n_chars].rsplit(' ', 1)[0])+1:], n_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des textes à résumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      " Karan Singh, Assistant Professor of Operations Research \n",
      "\n",
      "Characteristics common to both language models and supervised learning:\n",
      "1.Predicting Well is the Yardstick. A prediction rule is good as long\n",
      "as it makes reasonable predictions on average. Compared to more\n",
      "ambitious sub-disciplines in statistics, any statements about\n",
      "causality, p-values, and recovering latent structure are absent. We are\n",
      "similarly impervious to such considerations in language models. Such\n",
      "simplicity of goals enables very flexible prediction rules in machine\n",
      "learning. Although seeming modest in its aim, the art of machine\n",
      "learning has long been to cast as many disparate problems as questions\n",
      "about prediction as possible. Predicting house prices from square\n",
      "footage is a regular regression task. But, for reverse image\n",
      "captioning, is “predicting” a (high-dimensional) image given a few\n",
      "words a reasonable or well-defined classification task? Yet, this is\n",
      "how machine learning algorithms function. 2.Model Agnosticism.\n",
      "Supervised learning algorithms realize the adage that all models are\n",
      "wrong, but some are useful. For example, when building the price\n",
      "predictor above, a data scientist does not believe that the genuine\n",
      "relationship between prices and area is linear or well-specified.\n",
      "Similarly, when using neural networks to predict the next word in\n",
      "language models, we don’t believe that this is how Shakespeare must\n",
      "have employed a neural network to compose his texts. Yet, there are\n",
      "crucial differences:  5\n",
      "Figure 5: Predicting house prices from square\n",
      "footage. Pictured is a linear regression, an example of a supervised\n",
      "learning algorithm that uses extant data to learn a linear predictor.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pypdf import PdfReader\n",
    "\n",
    "# Empty list to store page text\n",
    "texts = []\n",
    "\n",
    "# Loop through each PDF file\n",
    "for file_name in os.listdir(work_directory):\n",
    "    \n",
    "    # Check if file is a PDF\n",
    "    if file_name.endswith('.pdf'):\n",
    "        \n",
    "        # Create PDF file object\n",
    "        reader = PdfReader(os.path.join(work_directory, file_name))\n",
    "        \n",
    "        # Loop through pages and extract text\n",
    "        for page in reader.pages:\n",
    "            \n",
    "            # Extract text from page\n",
    "            texts.append(page.extract_text())\n",
    "            \n",
    "print(len(texts))\n",
    "print(word_wrap(texts[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement de la Clé pour OpenAI\n",
    "\n",
    "Il vous faut obtenir d'Open AI une clé pour exécuter ce notebook Jupyter. Vous pouvez consulter [Where do I find my API key?](https://help.openai.com/en/articles/4936850-where-do-i-find-my-api-key). Ensuite, le chargement se fait soit à partir de l'environnement (fichier `.env`), soit à partir des secrets de Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "openai_api_key = None\n",
    "if 'google.colab' in sys.modules:\n",
    "  from google.colab import userdata\n",
    "  openai_api_key = userdata.get('OPENAI_API_KEY')\n",
    "else:\n",
    "  from dotenv import load_dotenv, find_dotenv\n",
    "  _ = load_dotenv(find_dotenv()) # read local .env file\n",
    "  openai_api_key  = os.environ['OPENAI_API_KEY']\n",
    "openai.api_key = openai_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Résumé de Texte avec LlamaIndex\n",
    "\n",
    "Cette approche utilise l'API Restful d'Open AI par le framework LlamaIndex \n",
    "\n",
    "Docs: https://docs.llamaindex.ai/en/stable/examples/index_structs/doc_summary/DocSummary.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jlchereau/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e50b64da3ac47e9b629925f677d6e60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd77cc9846d64db489d5983323b1d075",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarizing documents:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 5ca561cc-5f98-4365-8f08-fdea1efe9691\n",
      "current doc id: 2cf51eac-43f5-41f2-9f10-6bf704d0aeab\n",
      "current doc id: eb291e4c-fd52-415b-8bb3-dfa9c7ee4c9b\n",
      "current doc id: f5f41f56-119f-4171-9924-b9d2ce675296\n",
      "current doc id: 2854b57a-aaba-4acb-a593-54f1a1edc613\n",
      "current doc id: ecda5e70-284f-4475-b0da-0b436f42bf31\n",
      "current doc id: 0926d181-cc46-406a-8122-95a3e228b35c\n",
      "current doc id: 859c7d43-410c-436d-9e92-2c03e1bdbf67\n",
      "current doc id: 7b4e7b86-8314-4285-ab68-44e2376facd5\n",
      "current doc id: 858d5197-4988-4136-958e-20700de1a94d\n",
      "current doc id: 6d22e68d-b53f-4e8a-bb52-d9c2131e944c\n",
      "current doc id: 7034fa56-ef33-4907-b511-c9b0610deaae\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b222d669e87450eb85bfdb1d19c4cfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The context information mentions various details related to the\n",
      "performance and usage of GPT4, a language model developed by OpenAI. It\n",
      "highlights the tradeoffs between fine-tuning and in-context learning in\n",
      "such models. It also refers to the performance of GPT4 on academic,\n",
      "professional, and programming exams, with a specific mention of its\n",
      "performance on the bar exam. However, there are concerns raised by\n",
      "others regarding this performance. The context also includes references\n",
      "to industry and research leaders' opinions on GPT4. Additionally, it\n",
      "mentions the initial adoption statistics for ChatGPT, investments in\n",
      "GenAI, and the user bases for GenAI. The context briefly explains the\n",
      "use of Beam Search for producing text and prompt engineering for\n",
      "structuring initial text inputs. Finally, it provides links to papers\n",
      "related to various topics, including GPT3, Transformers, Word2Vec, and\n",
      "instruct GPT.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms import OpenAI\n",
    "from llama_index import (\n",
    "  SimpleDirectoryReader,\n",
    "  ServiceContext,\n",
    "  get_response_synthesizer,\n",
    ")\n",
    "from llama_index.indices.document_summary import DocumentSummaryIndex\n",
    "\n",
    "documents = SimpleDirectoryReader(work_directory).load_data()\n",
    "llm = OpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
    "service_context = ServiceContext.from_defaults(llm=llm, chunk_size=1024)\n",
    "response_synthesizer = get_response_synthesizer(response_mode=\"tree_summarize\", use_async=True)\n",
    "index = DocumentSummaryIndex.from_documents(\n",
    "    documents,\n",
    "    service_context=service_context,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    show_progress=True,\n",
    ")\n",
    "query_engine = index.as_query_engine()\n",
    "query = query_engine.query(\"Could you summarize the given context? Return your response which covers the key points of the text and does not miss anything important, please.\")\n",
    "print(word_wrap(query.response))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Résumé de Texte avec LangChain\n",
    "\n",
    "Cette approche utilise l'API Restful d'Open AI par le framework LangChain \n",
    "\n",
    "Docs: https://python.langchain.com/docs/use_cases/summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "documents = PyPDFDirectoryLoader(work_directory).load()\n",
    "llm_name = \"gpt-3.5-turbo\"\n",
    "llm = ChatOpenAI(model_name=llm_name, openai_api_key=openai_api_key, temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L'Approche Prompt (se heurte à la taille du context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 4370 tokens. Please reduce the length of the messages.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from openai import BadRequestError\n",
    "\n",
    "def format_docs(documents):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "\n",
    "template = \"Summarize the main themes in these retrieved docs: {docs}\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "chain = {\"docs\": format_docs} | prompt | llm | StrOutputParser()\n",
    "try:\n",
    "    chain.invoke(documents)\n",
    "except BadRequestError as e:\n",
    "    print(e.body['message'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L'Approche MapReduce\n",
    "\n",
    "Il s'agit de résumer chacun des documents, puis de résumer l'ensemble des résumés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generative artificial intelligence (GenAI) tools, specifically large\n",
      "language models (LLMs), have made significant progress in producing new\n",
      "content based on user prompts. These tools have the potential to\n",
      "enhance business processes and have attracted increased interest and\n",
      "investment. The report explores the principles and functions of LLMs,\n",
      "their impact, and future advances. It discusses the importance of the\n",
      "initial prompt, the development of modern LLMs, and the limitations of\n",
      "classical supervised learning. The article also highlights the\n",
      "advancements in representation learning through deep learning\n",
      "algorithms and the architecture of the GPT3 model. It introduces\n",
      "OpenAI's InstructGPT model and the concept of foundation models.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "chain = load_summarize_chain(llm, chain_type=\"map_reduce\", token_max=3000)\n",
    "response = chain.invoke(documents)\n",
    "print(word_wrap(response['output_text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L'Approche Refine\n",
    "\n",
    "Il s'agit de résumer chaque document avec le résumé des précédents, jusqu'à atteindre le dernier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generative artificial intelligence (GenAI) tools, such as large\n",
      "language models (LLMs), have made significant progress and are now\n",
      "comparable to human-level performance on academic and professional\n",
      "benchmarks. These tools have the potential to enhance business\n",
      "processes and enable new deliverables. This report primarily focuses on\n",
      "LLMs capable of generating novel text from textual prompts, but it also\n",
      "mentions the successful deployment of image- and code-based GenAI\n",
      "models by companies like Adobe and Github. The report highlights the\n",
      "importance of prompt engineering and the impact of the initial prompt\n",
      "on the quality and coherence of the generated text. It discusses the\n",
      "development of modern large language models and their differences from\n",
      "traditional predictive models. The report also explores the limitations\n",
      "of pre-deep-learning-era supervised algorithms and the revolution in\n",
      "deep learning that automated the process of representation learning. It\n",
      "introduces the concept of transformers, which can efficiently capture\n",
      "long-range relations between tokens, and discusses the training of\n",
      "increasingly larger language models. The report then introduces the\n",
      "concept of in-context learning, which allows for the repurposing of\n",
      "pre-trained language models for specific downstream tasks without the\n",
      "need for computationally expensive adjustments. It also mentions\n",
      "OpenAI's InstructGPT model, which aligns the model to follow users'\n",
      "instructions through fine-tuning and reinforcement learning. The report\n",
      "concludes by discussing the concept of foundation models, which aim to\n",
      "amortize the cost of limited-data downstream tasks by pre-training on\n",
      "large corpora of broadly related tasks or unlabelled datasets.\n",
      "Additionally, it provides further information on the performance and\n",
      "concerns related to GPT4, as well as the adoption and user bases of\n",
      "GenAI. It also mentions the use of Beam Search for producing text and\n",
      "the importance of prompt engineering. The report references various\n",
      "papers and research for more in-depth understanding of the topics\n",
      "discussed.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "chain = load_summarize_chain(llm, chain_type=\"refine\")\n",
    "response = chain.invoke(documents)\n",
    "print(word_wrap(response['output_text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Résumé de Texte avec Google T5\n",
    "\n",
    "Cette approche utilise directement un modèle de langage large en local.\n",
    "\n",
    "Docs:\n",
    "- https://huggingface.co/docs/transformers/tasks/summarization\n",
    "- https://huggingface.co/docs/transformers/model_doc/t5\n",
    "- https://www.analyticsvidhya.com/blog/2023/06/pdf-summarization-with-transformers-in-python/\n",
    "- https://blog.research.google/2022/03/auto-generated-summaries-in-google-docs.html\n",
    "- https://blog.research.google/2020/06/pegasus-state-of-art-model-for.html\n",
    "- https://medium.com/gopenai/text-summarization-using-flan-t5-5ded2e4ce182\n",
    "- https://medium.com/artificialis/t5-for-text-summarization-in-7-lines-of-code-b665c9e40771"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour supprimer le modèle du cache\n",
    "# !pip3 install -q -U \"huggingface_hub[cli]\"\n",
    "# !huggingface-cli delete-cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Initialisation du Modèle et de son Tokenizer\n",
    "\n",
    "Notez que les modèles T5-base ont environ 220 millions de paramètres, à comparer avec les environ 175 milliards de paramètres de GPT 3.5 (Open AI). On ne peut pas en attendre la même qualité de résumés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# https://huggingface.co/docs/transformers/model_doc/t5\n",
    "tr_name = 'T5-base'\n",
    "\n",
    "# https://huggingface.co/docs/transformers/model_doc/t5v1.1\n",
    "# tr_name = 'google/t5-v1_1-base'\n",
    "\n",
    "# https://huggingface.co/docs/transformers/model_doc/flan-t5\n",
    "# tr_name = 'google/flan-t5-base'\n",
    "\n",
    "# tokenizer = T5Tokenizer.from_pretrained(tr_name)\n",
    "# model = T5ForConditionalGeneration.from_pretrained(tr_name)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(tr_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(tr_name, return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Résumé d'une variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> data science is an interdisciplinary field focused on extracting\n",
      "knowledge from typically large data sets. it incorporates skills from\n",
      "computer science, statistics, information science, mathematics, data\n",
      "visualization, information visualization, data sonification, graphic\n",
      "design, complex systems, communication and business. the field\n",
      "encompasses preparing data for analysis, formulating data science\n",
      "problems, analyzing data, developing data-driven solutions.</s>\n"
     ]
    }
   ],
   "source": [
    "text = (\"Data science is an interdisciplinary field[10] focused on extracting knowledge from typically large data sets and applying the knowledge and insights from that data to solve problems in a wide range of application domains.[11] The field encompasses preparing data for analysis, formulating data science problems, analyzing data, developing data-driven solutions, and presenting findings to inform high-level decisions in a broad range of application domains. As such, it incorporates skills from computer science, statistics, information science, mathematics, data visualization, information visualization, data sonification, data integration, graphic design, complex systems, communication and business.[12][13] Statistician Nathan Yau, drawing on Ben Fry, also links data science to human–computer interaction: users should be able to intuitively control and explore data.[14][15] In 2015, the American Statistical Association identified database management, statistics and machine learning, and distributed and parallel systems as the three emerging foundational professional communities.[16]\")\n",
    "inputs = tokenizer.encode(\"sumarize: \" + text, return_tensors='pt', max_length=512, truncation=True)\n",
    "output = model.generate(inputs, min_length=80, max_length=100)\n",
    "summary = tokenizer.decode(output[0])\n",
    "print(word_wrap(summary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Résumé de Textes par l'Approche MapReduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [09:39<00:00, 48.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> openAI’s chatGPT, a conversational web app based on a generative\n",
      "(multimodal) language model, took five days to reach one million users.\n",
      "on the business side, the number of jobs mentioning AI-related skills\n",
      "quadrupled from 2022 to 2023.\n",
      "­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­\n",
      "­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­\n",
      "­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­\n",
      "­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­\n",
      "­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­\n",
      "­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­\n",
      "­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­\n",
      "­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­\n",
      "­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­\n",
      "­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­\n",
      "­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­\n",
      "­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­\n",
      "­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "summaries = []\n",
    "\n",
    "def summarize(text):\n",
    "    inputs = tokenizer.encode(\"summarize: \" + text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    outputs = model.generate(inputs, max_new_tokens=1000, min_new_tokens=250, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    return tokenizer.decode(outputs[0])    \n",
    "\n",
    "for text in tqdm(texts):\n",
    "    summaries.append(summarize(text))\n",
    "\n",
    "summary = summarize(\"\\n\\n\".join(summaries))\n",
    "print(word_wrap(summary))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
