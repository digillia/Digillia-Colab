{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Résumé de Textes\n",
    "\n",
    "Nous démontrons ici différentes méthodes simples pour résumer des documents en utilisant des modèles de langage larges.\n",
    "\n",
    "[![Index](https://img.shields.io/badge/Index-blue)](../index.ipynb)\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/digillia/Digillia-Colab/blob/main/use-cases/text-summarization.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Supprimer les commentaires pour installer (requirements.txt)\n",
    "# !pip3 install -q -U python-dotenv\n",
    "# !pip3 install -q -U torch\n",
    "# !pip3 install -q -U tqdm\n",
    "# !pip3 install -q -U transformers\n",
    "\n",
    "# À installer dans tous les cas pour Google Colab et Github\n",
    "if 'google.colab' in sys.modules or 'CI' in os.environ:\n",
    "  # BEGIN\n",
    "  # llmx cause de multiples problèmes de dépendances avec les librairies installées ensuite\n",
    "  # see https://community.openai.com/t/error-while-importing-openai-from-open-import-openai/578166\n",
    "  # see https://stackoverflow.com/questions/77759146/issue-installing-openai-in-colab\n",
    "  !pip3 uninstall llmx -y\n",
    "  # END\n",
    "  !pip3 install -q -U langchain\n",
    "  !pip3 install -q -U langchain-openai\n",
    "  !pip3 install -q -U langchain-community\n",
    "  !pip3 install -q -U llama-index\n",
    "  !pip3 install -q -U openai\n",
    "  !pip3 install -q -U pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cette variable python est accessible depuis les commandes bash\n",
    "work_directory = \"text-summarization\"\n",
    "\n",
    "# Récupération des données pour Google Colab et Github\n",
    "if 'google.colab' in sys.modules or 'CI' in os.environ:\n",
    "  !curl --create-dirs -O --output-dir $work_directory \"https://raw.githubusercontent.com/digillia/Digillia-Colab/main/use-cases/text-summarization/sample_1.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction utilitaire pour afficher du texte sur plusieurs lignes\n",
    "def word_wrap(string, n_chars=72):\n",
    "    if len(string) < n_chars:\n",
    "        return string\n",
    "    else:\n",
    "        return string[:n_chars].rsplit(' ', 1)[0] + '\\n' + word_wrap(string[len(string[:n_chars].rsplit(' ', 1)[0])+1:], n_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des textes à résumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      " \n",
      "Karan Singh, Assistant Professor of Operations Research \n",
      "\n",
      "\n",
      "Characteristics common to both language models and supervised\n",
      "learning: \n",
      "1. Predicting Well is the Yardstick. A prediction rule is\n",
      "good as long as it makes \n",
      "reasonable predictions on average. Compared\n",
      "to more ambitious sub-disciplines in \n",
      "statistics, any statements about\n",
      "causality, p-values, and recovering latent structure are \n",
      "absent. We\n",
      "are similarly impervious to such considerations in language models.\n",
      "Such \n",
      "simplicity of goals enables very flexible prediction rules in\n",
      "machine learning. Although \n",
      "seeming modest in its aim, the art of\n",
      "machine learning has long been to cast as many \n",
      "disparate problems as\n",
      "questions about prediction as possible. Predicting house prices \n",
      "from\n",
      "square footage is a regular regression task. But, for reverse image\n",
      "captioning, is \n",
      "“predicting” a (high-dimensional) image given a few\n",
      "words a reasonable or well-defined \n",
      "classification task? Yet, this is\n",
      "how machine learning algorithms function. \n",
      "2. Model Agnosticism.\n",
      "Supervised learning algorithms realize the adage that all models \n",
      "are\n",
      "wrong, but some are useful. For example, when building the price\n",
      "predictor above, a \n",
      "data scientist does not believe that the genuine\n",
      "relationship between prices and area is \n",
      "linear or well-specified.\n",
      "Similarly, when using neural networks to predict the next word in\n",
      "\n",
      "language models, we don’t believe that this is how Shakespeare must\n",
      "have employed a \n",
      "neural network to compose his texts. \n",
      "Yet, there are\n",
      "crucial differences: \n",
      " 5\n",
      "Figure 5: Predicting house prices from square\n",
      "footage. Pictured is a linear \n",
      "regression, an example of a supervised\n",
      "learning algorithm that uses extant \n",
      "data to learn a linear predictor.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pypdf import PdfReader\n",
    "\n",
    "# Empty list to store page text\n",
    "texts = []\n",
    "\n",
    "# Loop through each PDF file\n",
    "for file_name in os.listdir(work_directory):\n",
    "    \n",
    "    # Check if file is a PDF\n",
    "    if file_name.endswith('.pdf'):\n",
    "        \n",
    "        # Create PDF file object\n",
    "        reader = PdfReader(os.path.join(work_directory, file_name))\n",
    "        \n",
    "        # Loop through pages and extract text\n",
    "        for page in reader.pages:\n",
    "            \n",
    "            # Extract text from page\n",
    "            texts.append(page.extract_text())\n",
    "            \n",
    "print(len(texts))\n",
    "print(word_wrap(texts[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement de la Clé pour OpenAI\n",
    "\n",
    "Il vous faut obtenir d'Open AI une clé pour exécuter ce notebook Jupyter. Vous pouvez consulter [Where do I find my API key?](https://help.openai.com/en/articles/4936850-where-do-i-find-my-api-key). Ensuite, le chargement se fait soit à partir de l'environnement (fichier `.env`), soit à partir des secrets de Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "openai_api_key = None\n",
    "if 'google.colab' in sys.modules:\n",
    "  from google.colab import userdata\n",
    "  openai_api_key = userdata.get('OPENAI_API_KEY')\n",
    "else:\n",
    "  from dotenv import load_dotenv, find_dotenv\n",
    "  _ = load_dotenv(find_dotenv()) # read local .env file\n",
    "  openai_api_key  = os.environ['OPENAI_API_KEY']\n",
    "openai.api_key = openai_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Résumé de Texte avec LlamaIndex\n",
    "\n",
    "Cette approche utilise l'API Restful d'Open AI par le framework LlamaIndex \n",
    "\n",
    "Docs: https://docs.llamaindex.ai/en/stable/examples/index_structs/doc_summary/DocSummary.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a06aa1ba100e49339c1ec9c3e683d657",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d55e3487b0649ba9126c53586364b5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarizing documents:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: c9029ca0-5428-43ec-9c29-6086ca915d1f\n",
      "current doc id: 2a7cfa6d-6676-4c2d-a783-b85123da7767\n",
      "current doc id: 0950fedd-fa57-4c1c-8a6a-36dcbad2d63c\n",
      "current doc id: 48589af2-16d1-4cd0-a820-39c218b918e0\n",
      "current doc id: 1985f9b5-ef73-433c-b6af-6051932660cf\n",
      "current doc id: 0a0f0ee5-d551-4d06-8e3f-80cff29284c5\n",
      "current doc id: d05317dd-a324-4b24-8aa6-01e7c6ff0372\n",
      "current doc id: 462e16e5-35be-4067-92d7-c1ce2ddbdb12\n",
      "current doc id: a119bb38-bc7d-491f-a888-c3f68c771e58\n",
      "current doc id: f3633939-6b76-4cbd-a1fe-40b2b2628f92\n",
      "current doc id: 3e825353-5d1c-484e-b33a-1b9c9c14e2b8\n",
      "current doc id: 3528b425-eed1-458a-9234-de6f2275f4ca\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fefaac60eb54819b105723c66ad8626",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language models predict the next word based on a context window of\n",
      "preceding words by outputting a probability distribution over all\n",
      "possible words. This randomness in predictions allows for varied\n",
      "completions on different runs. Sampling the next word probabilistically\n",
      "is crucial for generating high-quality text, as opposed to choosing the\n",
      "most likely word greedily. This stochastic process enables models like\n",
      "ChatGPT to offer diverse responses to the same prompt.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core import (\n",
    "  SimpleDirectoryReader,\n",
    "  get_response_synthesizer,\n",
    ")\n",
    "from llama_index.core.indices import DocumentSummaryIndex\n",
    "\n",
    "documents = SimpleDirectoryReader(work_directory).load_data()\n",
    "llm = OpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
    "embedding_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
    "response_synthesizer = get_response_synthesizer(response_mode=\"tree_summarize\", use_async=True)\n",
    "index = DocumentSummaryIndex.from_documents(\n",
    "    documents,\n",
    "    llm=llm,\n",
    "    embedding_model=embedding_model,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    show_progress=True,\n",
    ")\n",
    "query_engine = index.as_query_engine()\n",
    "query = query_engine.query(\"Could you summarize the given context? Return your response which covers the key points of the text and does not miss anything important, please.\")\n",
    "print(word_wrap(query.response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Résumé de Texte avec LangChain\n",
    "\n",
    "Cette approche utilise l'API Restful d'Open AI par le framework LangChain \n",
    "\n",
    "Docs: https://python.langchain.com/docs/use_cases/summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "documents = PyPDFDirectoryLoader(work_directory).load()\n",
    "llm_name = \"gpt-3.5-turbo\"\n",
    "llm = ChatOpenAI(model_name=llm_name, openai_api_key=openai_api_key, temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L'Approche Prompt (se heurte à la taille du contexte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The main themes in these documents include the introduction and\n",
      "explanation of generative artificial intelligence (GenAI) tools, the\n",
      "impact of these tools on business processes and operations, the\n",
      "technical aspects of language models, the advancements in machine\n",
      "learning and artificial intelligence that have led to the development\n",
      "of large language models, the use of word embeddings and contrastive\n",
      "learning in creating representations for textual data, the role of\n",
      "transformers in capturing long-range relationships in text, the concept\n",
      "of in-context learning for repurposing pre-trained language models for\n",
      "specific tasks, and the potential future applications of foundation\n",
      "models in various domains. The documents also discuss the\n",
      "democratization of using large language models like GPT3 through\n",
      "in-context learning and the importance of prompt engineering in\n",
      "eliciting useful outputs from these models.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from openai import BadRequestError\n",
    "\n",
    "def format_docs(documents):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "\n",
    "template = \"Summarize the main themes in these retrieved docs: {docs}\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "chain = {\"docs\": format_docs} | prompt | llm | StrOutputParser()\n",
    "try:\n",
    "    response = chain.invoke(documents)\n",
    "    print(word_wrap(response))\n",
    "except BadRequestError as e:\n",
    "    print(e.body['message'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L'Approche MapReduce\n",
    "\n",
    "Il s'agit de résumer chacun des documents, puis de résumer l'ensemble des résumés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generative artificial intelligence (GenAI) tools are a new class of AI\n",
      "algorithms that can create novel content based on user prompts, with\n",
      "recent advancements in machine learning leading to human-level\n",
      "performance. This has the potential to revolutionize business processes\n",
      "and create new opportunities, attracting significant investment and\n",
      "interest. The report by Karan Singh explores large language models\n",
      "(LLMs) like GPT3, discussing their principles, functions, and impact,\n",
      "as well as the importance of initial prompts and supervised learning.\n",
      "The evolution of neural network architectures from RNNs to Transformers\n",
      "has improved language models, with models like GPT3 showing high\n",
      "performance but high training costs. In-context learning approaches\n",
      "like InstructGPT aim to address limitations in following instructions.\n",
      "The future of AI lies in foundation models like GPT4, with tradeoffs in\n",
      "performance-resource balance.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "chain = load_summarize_chain(llm, chain_type=\"map_reduce\", token_max=3000)\n",
    "response = chain.invoke(documents)\n",
    "print(word_wrap(response['output_text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L'Approche Refine\n",
    "\n",
    "Il s'agit de résumer chaque document avec le résumé des précédents, jusqu'à atteindre le dernier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The discussion by Karan Singh, Assistant Professor of Operations\n",
      "Research, on contrastive learning and the evolution of neural network\n",
      "architectures highlights the practicality and effectiveness of\n",
      "pre-training large language models like GPT2 and GPT3 on auxiliary\n",
      "tasks before fine-tuning them on specific labeled examples for\n",
      "downstream tasks. The introduction of transformers has revolutionized\n",
      "the field by efficiently capturing long-range relations between tokens.\n",
      "The paper on GPT3's architecture introduces in-context learning as a\n",
      "cost-effective way to repurpose pre-trained language models for\n",
      "specific tasks, making them more accessible to end-users. While\n",
      "fine-tuning may still offer performance gains, GPT3 significantly\n",
      "reduces the computational load, democratizing the use of large language\n",
      "models. The report also discusses the development of InstructGPT, which\n",
      "aligns models to follow user instructions through supervised and\n",
      "reinforcement learning, and the concept of foundation models that aim\n",
      "to generalize the success of large language models to other domains.\n",
      "This broader perspective helps users understand the origins and\n",
      "differences of large language models within the machine learning and\n",
      "artificial intelligence landscape, taking into account the\n",
      "performance-resource tradeoffs between fine-tuning and in-context\n",
      "learning.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "chain = load_summarize_chain(llm, chain_type=\"refine\")\n",
    "response = chain.invoke(documents)\n",
    "print(word_wrap(response['output_text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Résumé de Texte avec Google T5\n",
    "\n",
    "Cette approche utilise directement un modèle de langage large en local.\n",
    "\n",
    "Docs:\n",
    "- https://huggingface.co/docs/transformers/tasks/summarization\n",
    "- https://huggingface.co/docs/transformers/model_doc/t5\n",
    "- https://www.analyticsvidhya.com/blog/2023/06/pdf-summarization-with-transformers-in-python/\n",
    "- https://blog.research.google/2022/03/auto-generated-summaries-in-google-docs.html\n",
    "- https://blog.research.google/2020/06/pegasus-state-of-art-model-for.html\n",
    "- https://medium.com/gopenai/text-summarization-using-flan-t5-5ded2e4ce182\n",
    "- https://medium.com/artificialis/t5-for-text-summarization-in-7-lines-of-code-b665c9e40771"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour supprimer le modèle du cache\n",
    "# !pip3 install -q -U \"huggingface_hub[cli]\"\n",
    "# !huggingface-cli delete-cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Initialisation du Modèle et de son Tokenizer\n",
    "\n",
    "Notez que les modèles T5-base ont environ 220 millions de paramètres, à comparer avec les environ 175 milliards de paramètres de GPT 3.5 (Open AI). On ne peut pas en attendre la même qualité de résumés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d45b4c67ea884edb8cf4ee82eec1972d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caed6f7b1aff40d9ada02deba810253d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e2528d579ac4523ae95b9b8e2cfdd80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "273accc566084eb19212cbcc978272ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "444c47fa8ce743f89d288879198c861a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# https://huggingface.co/docs/transformers/model_doc/t5\n",
    "tr_name = 'T5-base'\n",
    "\n",
    "# https://huggingface.co/docs/transformers/model_doc/t5v1.1\n",
    "# tr_name = 'google/t5-v1_1-base'\n",
    "\n",
    "# https://huggingface.co/docs/transformers/model_doc/flan-t5\n",
    "# tr_name = 'google/flan-t5-base'\n",
    "\n",
    "# tokenizer = T5Tokenizer.from_pretrained(tr_name)\n",
    "# model = T5ForConditionalGeneration.from_pretrained(tr_name)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(tr_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(tr_name, return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Résumé d'une variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> data science is an interdisciplinary field focused on extracting\n",
      "knowledge from typically large data sets . it incorporates skills from\n",
      "computer science, statistics, information science, mathematics, data\n",
      "visualization, information visualization, data sonification, graphic\n",
      "design, complex systems, communication and business . the field\n",
      "encompasses preparing data for analysis, formulating data science\n",
      "problems, analyzing data, developing data-driven solutions .</s>\n"
     ]
    }
   ],
   "source": [
    "text = (\"Data science is an interdisciplinary field[10] focused on extracting knowledge from typically large data sets and applying the knowledge and insights from that data to solve problems in a wide range of application domains.[11] The field encompasses preparing data for analysis, formulating data science problems, analyzing data, developing data-driven solutions, and presenting findings to inform high-level decisions in a broad range of application domains. As such, it incorporates skills from computer science, statistics, information science, mathematics, data visualization, information visualization, data sonification, data integration, graphic design, complex systems, communication and business.[12][13] Statistician Nathan Yau, drawing on Ben Fry, also links data science to human–computer interaction: users should be able to intuitively control and explore data.[14][15] In 2015, the American Statistical Association identified database management, statistics and machine learning, and distributed and parallel systems as the three emerging foundational professional communities.[16]\")\n",
    "inputs = tokenizer.encode(\"sumarize: \" + text, return_tensors='pt', max_length=512, truncation=True)\n",
    "output = model.generate(inputs, min_length=80, max_length=100)\n",
    "summary = tokenizer.decode(output[0])\n",
    "print(word_wrap(summary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Résumé de Textes par l'Approche MapReduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [06:38<00:00, 33.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> recent advances in machine learning (ML), massive datasets, and\n",
      "substantial increases in computing power have propelled such tools to\n",
      "human-level performance on academic and professional benchmarks .\n",
      "­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­\n",
      "­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­\n",
      "­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­\n",
      "­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­\n",
      "­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­\n",
      "­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­\n",
      "­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­\n",
      "­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­\n",
      "­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­\n",
      "­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­\n",
      "­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­\n",
      "­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­\n",
      "­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­\n",
      "­­­­­­­­­­­­­­\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "summaries = []\n",
    "\n",
    "def summarize(text):\n",
    "    inputs = tokenizer.encode(\"summarize: \" + text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    outputs = model.generate(inputs, max_new_tokens=1000, min_new_tokens=250, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    return tokenizer.decode(outputs[0])    \n",
    "\n",
    "for text in tqdm(texts):\n",
    "    summaries.append(summarize(text))\n",
    "\n",
    "summary = summarize(\"\\n\\n\".join(summaries))\n",
    "print(word_wrap(summary))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
