{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Résumé de Textes\n",
    "\n",
    "Nous démontrons ici différentes méthodes simples pour résumer des documents en utilisant des modèles de langage larges.\n",
    "\n",
    "[![Index](https://img.shields.io/badge/Index-blue)](../index.ipynb)\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/digillia/Digillia-Colab/blob/main/use-cases/text-summarization.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "\n",
    "# Supprimer les commentaires pour installer\n",
    "# !pip3 install -q -U python-dotenv\n",
    "# !pip3 install -q -U torch\n",
    "# !pip3 install -q -U tqdm\n",
    "# !pip3 install -q -U transformers\n",
    "\n",
    "# À installer dans tous les cas pour Google Colab\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "  # BEGIN\n",
    "  # llmx cause de multiples problèmes de dépendances avec les librairies installées ensuite\n",
    "  # see https://community.openai.com/t/error-while-importing-openai-from-open-import-openai/578166\n",
    "  # see https://stackoverflow.com/questions/77759146/issue-installing-openai-in-colab\n",
    "  !pip3 uninstall llmx -y\n",
    "  # END\n",
    "  !pip3 install -q -U langchain\n",
    "  !pip3 install -q -U langchain-openai\n",
    "  !pip3 install -q -U llama-index\n",
    "  !pip3 install -q -U openai\n",
    "  !pip3 install -q -U pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This python variable cab be accessed by bash commands\n",
    "pdf_directory = \"text-summarization\"\n",
    "\n",
    "# Récupération des données pour Google Colab\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "  !curl --create-dirs -O --output-dir $pdf_directory \"https://raw.githubusercontent.com/digillia/Digillia-Colab/main/use-cases/text-summarization/sample_1.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des textes à résumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c567eec25d58405b8e3184d6a2f562f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value=' Karan Singh, Assistant Professor of Operations Research \\n Characteristics common to both languag…"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pypdf import PdfReader\n",
    "from ipywidgets import HTML\n",
    "\n",
    "# Empty list to store page text\n",
    "texts = []\n",
    "\n",
    "# Loop through each PDF file\n",
    "for file_name in os.listdir(pdf_directory):\n",
    "    \n",
    "    # Check if file is a PDF\n",
    "    if file_name.endswith('.pdf'):\n",
    "        \n",
    "        # Create PDF file object\n",
    "        reader = PdfReader(os.path.join(pdf_directory, file_name))\n",
    "        \n",
    "        # Loop through pages and extract text\n",
    "        for page in reader.pages:\n",
    "            \n",
    "            # Extract text from page\n",
    "            texts.append(page.extract_text())\n",
    "            \n",
    "print(len(texts))\n",
    "HTML(texts[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement de Clé OpenAI\n",
    "\n",
    "Il vous faut obtenir d'Open AI une clé pour exécuter ce notebook Jupyter. Vous pouvez consulter [Where do I find my API key?](https://help.openai.com/en/articles/4936850-where-do-i-find-my-api-key). Ensuite, le chargement se fait soit à partir de l'environnement (fichier `.env`), soit à partir des secrets de Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "  from google.colab import userdata\n",
    "  openai.api_key = userdata.get('OPENAI_API_KEY')\n",
    "else:\n",
    "  from dotenv import load_dotenv, find_dotenv\n",
    "  _ = load_dotenv(find_dotenv()) # read local .env file\n",
    "  openai.api_key  = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Résumé de Texte avec LlamaIndex\n",
    "\n",
    "Cette approche utilise l'API Restful d'Open AI par le framework LlamaIndex \n",
    "\n",
    "Docs: https://docs.llamaindex.ai/en/stable/examples/index_structs/doc_summary/DocSummary.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20a32256fcc64e2cba067d68ebf86879",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69e8707056e74e62b8c18f5368a4ee8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarizing documents:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: fe666552-335e-4ce8-9a2b-e019b4785e87\n",
      "current doc id: cd1d91e7-4f7f-4373-90fb-ee97288dc9cc\n",
      "current doc id: 068c8817-e176-4deb-be42-d48097e3e67d\n",
      "current doc id: 1279b706-57d0-4c95-a3dc-724161271427\n",
      "current doc id: 58fd1974-f14f-4120-b5ae-3ed2521770b5\n",
      "current doc id: ba05641a-8893-4526-9729-79c2f392f882\n",
      "current doc id: c5ab2fc9-e73e-4da2-bffb-c9bb14b7ae92\n",
      "current doc id: 3d952e4a-1489-4f26-9790-7a82ba15ed22\n",
      "current doc id: e423d7be-aae9-4008-b8c7-fdfdfcea2623\n",
      "current doc id: 64b8a6a1-de0a-46f8-943a-c8d07cf51e26\n",
      "current doc id: 0e72056f-f811-4b66-9d7c-bbf771cb93c1\n",
      "current doc id: baefa322-e3bc-4593-9eb1-a3fe00c2e8da\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7a1fc687ccd4a92aebc357439996327",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49d124c8dd7d4284a7e7ea1c723e92f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value=\"The context information mentions various details related to the performance and tradeoffs of GPT4,…"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.llms import OpenAI\n",
    "from llama_index import (\n",
    "  SimpleDirectoryReader,\n",
    "  ServiceContext,\n",
    "  get_response_synthesizer,\n",
    ")\n",
    "from llama_index.indices.document_summary import DocumentSummaryIndex\n",
    "from ipywidgets import HTML\n",
    "\n",
    "documents = SimpleDirectoryReader(pdf_directory).load_data()\n",
    "llm = OpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
    "service_context = ServiceContext.from_defaults(llm=llm, chunk_size=1024)\n",
    "response_synthesizer = get_response_synthesizer(response_mode=\"tree_summarize\", use_async=True)\n",
    "index = DocumentSummaryIndex.from_documents(\n",
    "    documents,\n",
    "    service_context=service_context,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    show_progress=True,\n",
    ")\n",
    "query_engine = index.as_query_engine()\n",
    "query = query_engine.query(\"Could you summarize the given context? Return your response which covers the key points of the text and does not miss anything important, please.\")\n",
    "HTML(query.response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Résumé de Texte avec LangChain\n",
    "\n",
    "Cette approche utilise l'API Restful d'Open AI par le framework LangChain \n",
    "\n",
    "Docs: https://python.langchain.com/docs/use_cases/summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "documents = PyPDFDirectoryLoader(pdf_directory).load()\n",
    "llm_name = \"gpt-3.5-turbo\"\n",
    "llm = ChatOpenAI(model_name=llm_name, temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L'Approche Prompt (se heurte à la taille du context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 4370 tokens. Please reduce the length of the messages.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from openai import BadRequestError\n",
    "\n",
    "def format_docs(documents):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "\n",
    "template = \"Summarize the main themes in these retrieved docs: {docs}\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "chain = {\"docs\": format_docs} | prompt | llm | StrOutputParser()\n",
    "try:\n",
    "    chain.invoke(documents)\n",
    "except BadRequestError as e:\n",
    "    print(e.body['message'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L'Approche MapReduce\n",
    "\n",
    "Il s'agit de résumer chacun des documents, puis de résumer l'ensemble des résumés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91a3b4de3ae04382bb4fd90ea44cf4ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='Generative artificial intelligence (GenAI) tools, specifically large language models (LLMs), have …"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from ipywidgets import HTML\n",
    "\n",
    "chain = load_summarize_chain(llm, chain_type=\"map_reduce\", token_max=3000)\n",
    "response = chain.invoke(documents)\n",
    "HTML(response['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L'Approche Refine\n",
    "\n",
    "Il s'agit de résumer chaque document avec le résumé des précédents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c77a19ca93fd4f7ea7765ae0bc6bd2b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value=\"Generative artificial intelligence (GenAI) tools, such as large language models (LLMs), have made …"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from ipywidgets import HTML\n",
    "\n",
    "chain = load_summarize_chain(llm, chain_type=\"refine\")\n",
    "response = chain.invoke(documents)\n",
    "HTML(response['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Résumé de Texte avec Google T5\n",
    "\n",
    "Cette approche utilise directement un modèle de langage large en local.\n",
    "\n",
    "Docs:\n",
    "- https://huggingface.co/docs/transformers/tasks/summarization\n",
    "- https://huggingface.co/docs/transformers/model_doc/t5\n",
    "- https://www.analyticsvidhya.com/blog/2023/06/pdf-summarization-with-transformers-in-python/\n",
    "- https://blog.research.google/2022/03/auto-generated-summaries-in-google-docs.html\n",
    "- https://blog.research.google/2020/06/pegasus-state-of-art-model-for.html\n",
    "- https://medium.com/gopenai/text-summarization-using-flan-t5-5ded2e4ce182\n",
    "- https://medium.com/artificialis/t5-for-text-summarization-in-7-lines-of-code-b665c9e40771"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour vider le cache du model\n",
    "# !pip3 install -q -U \"huggingface_hub[cli]\"\n",
    "# huggingface-cli delete-cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Initialisation du Modèle et de son Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# https://huggingface.co/docs/transformers/model_doc/t5\n",
    "tr_name = 'T5-base'\n",
    "\n",
    "# https://huggingface.co/docs/transformers/model_doc/t5v1.1\n",
    "# tr_name = 'google/t5-v1_1-base'\n",
    "\n",
    "# tokenizer = T5Tokenizer.from_pretrained(tr_name)\n",
    "# model = T5ForConditionalGeneration.from_pretrained(tr_name)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(tr_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(tr_name, return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Résumé d'une variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ee2f1674a964987ad12563b09cf545a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<pad> data science is an interdisciplinary field focused on extracting knowledge from typically la…"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import HTML\n",
    "\n",
    "text = (\"Data science is an interdisciplinary field[10] focused on extracting knowledge from typically large data sets and applying the knowledge and insights from that data to solve problems in a wide range of application domains.[11] The field encompasses preparing data for analysis, formulating data science problems, analyzing data, developing data-driven solutions, and presenting findings to inform high-level decisions in a broad range of application domains. As such, it incorporates skills from computer science, statistics, information science, mathematics, data visualization, information visualization, data sonification, data integration, graphic design, complex systems, communication and business.[12][13] Statistician Nathan Yau, drawing on Ben Fry, also links data science to human–computer interaction: users should be able to intuitively control and explore data.[14][15] In 2015, the American Statistical Association identified database management, statistics and machine learning, and distributed and parallel systems as the three emerging foundational professional communities.[16]\")\n",
    "inputs = tokenizer.encode(\"sumarize: \" + text, return_tensors='pt', max_length=512, truncation=True)\n",
    "output = model.generate(inputs, min_length=80, max_length=100)\n",
    "summary = tokenizer.decode(output[0])\n",
    "HTML(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Résumé de Textes par l'Approche MapReduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/12 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from ipywidgets import HTML\n",
    "\n",
    "summaries = []\n",
    "\n",
    "def summarize(text):\n",
    "    inputs = tokenizer.encode(\"summarize: \" + text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    outputs = model.generate(inputs, max_length=1000, min_length=250, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    return tokenizer.decode(outputs[0])    \n",
    "\n",
    "for text in tqdm(texts):\n",
    "    summaries.append(summarize(text))\n",
    "\n",
    "summary = summarize(\"\\n\\n\".join(summaries))\n",
    "HTML(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
